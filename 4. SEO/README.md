# SEO

---

## 1. 🤔 SEO가 무엇일까요?

SEO(Search Engine Optimization, 검색엔진 최적화)란 [구글 크롤러](https://www.google.com/intl/ko/search/howsearchworks/crawling-indexing/)가 웹사이트의 정보를 크롤링 하고, 순위를 매기는 방식에 맞도록 웹사이트를 수정, 조율을 하여 최적화 작업을 하고, 이를 통해 검색 결과의 상단에 타겟 웹사이트를 구글 상위 노출을 시켜 트래픽(방문자)을 높이는 디지털 마케팅 전략을 지칭합니다.

## 2. 🤔 SEO는 왜 중요할까요?

마케팅의 한 종류로서 자신의 쇼핑몰 사이트나, 회사 사이트 같은 경우 포털 사이트의 **상위 노출**을 통해서 유저 수를 늘리거나, 트래픽을 늘립니다. 보통 쇼핑몰 같은 사이트들은 트래픽이 곧 매출이기 때문에 정말 중요하다고 볼 수 있습니다.

그리고 블로그와 같은 컨텐츠를 제공하는 사이트들도 자신의 컨텐츠가 상위 노출이 잘 되어서 자신의 사이트를 방문하는 사람을 늘릴 수 있습니다.

## 3. 🤔 Open Graph가 무엇인가요?

[Open Graph](https://ogp.me/)는 원래 Facebook에서 페이지 콘텐츠를 표현하기 위해 웹페이지 내 **메타데이터 사용을 표준화**하기 위해 만든 인터넷 프로토콜입니다.

그 안에는 페이지 제목처럼 간단하거나 동영상 길이만큼 구체적인 세부정보를 제공할 수 있습니다. 이 조각들은 모두 모여서 인터넷의 각 개별 페이지를 나타냅니다.

**Open Graph 언제 쓰나요?**

SNS을 통해 해당 링크를 복사하여 SNS에 개시를 하면 링크를 클릭해서 들어가기 전까지 이 링크가 어떤 데이터를 가지고 있는지 알 수가 없습니다.

예를 들면, SNS에 블로그를 알리기 위해, url 링크를 개시하면 단순히 url만으로는 개발 관련 사이트인지, 성인 사이트인지 알 수 없습니다. 또 지금같이 악성 코드가 범람하는 시대에 아무 의심없이 링크를 클릭해서 들어가기도 망설여 집니다.

그럴 때 사용하는 태그로써 미리보기 이미지나 그 링크에 대한 설명, 제목을 표시하기 위한 태그입니다.

**Open Graph가 구글 검색 순위를 올리는데 도움이 되나요?**

Google 검색 랭크에는 다른 평판이 높은 사이트에 얼마나 링크가 되어있는지에 따라 블로그의 순위가 결정된다고 되어있습니다. 즉, Open Graph 자체로는 구글 검색 엔진에 미치는 영향은 없지만 여러 SNS에 등록이 되어서 많은 사람들이 **그 링크를 타고 오게 되면 2차적으로 구글 검색 순위**에 도움이 된다고 할 수 있습니다. (출처 : [https://nowonbun.tistory.com/517](https://nowonbun.tistory.com/517) )

## 4. 🤔 sitemap.xml은 무엇인가요?

**_사이트맵_**은 사이트에 있는 페이지, 동영상 및 기타 파일과 그 관계에 관한 정보를 제공하는 파일입니다. Google과 같은 검색엔진은 이 파일을 읽고 사이트를 더 효율적으로 크롤링합니다. 사이트맵은 내가 사이트에서 중요하다고 생각하는 페이지와 파일을 크롤러에 알리고 중요한 관련 정보를 제공합니다. 관련 정보의 예로는 페이지가 마지막으로 업데이트된 시간, 페이지의 대체 언어 버전이 있습니다.

[사이트맵이란 무엇인가요? | Google 검색 센터 | 문서 | Google Developers](https://developers.google.com/search/docs/advanced/sitemaps/overview?hl=ko)

[사이트맵](https://en.wikipedia.org/wiki/Sitemaps) 은 검색로봇에게 사이트 내에 수집되어야 할 페이지들을 알려 주기 위하여 마련된 표준 규약입니다. 사이트맵을 활용하여 URL의 추가 정보를 검색로봇에 제공할 수 있으므로 검색로봇이 사이트의 콘텐츠를 더 잘 수집할 수 있도록 도울 수 있습니다.

사이트 맵은 본문이 아닌 콘텐츠의 URL 정보만 담고 있기 때문에 사이트 내의 모든 URL을 포함하는 것을 권장합니다. 검색로봇은 해당 사이트맵에 포함된 URL 정보를 추출후 내부 알고리즘을 통하여 수집 대상 URL을 선별하여 우선 순위별로 수집을 진행합니다.

[RSS 및 사이트맵 제출](https://searchadvisor.naver.com/guide/request-feed)

## 5. 🤔 robots.txt는 무엇인가요?

**robots.txt** 파일은 크롤러가 **사이트에서 액세스할 수 있는 URL**을 검색엔진 크롤러에 알려 줍니다. 이 파일은 주로 요청으로 인해 사이트가 오버로드되는 것을 방지하기 위해 사용합니다.

robots.txt 파일의 지침은 사이트에서의 크롤러 동작을 강제로 제어할 수 없습니다. 크롤러가 지침을 준수할지를 스스로 판단하게 됩니다. Googlebot 및 기타 잘 제작된 웹 크롤러는 robots.txt 파일의 지침을 준수하지만 준수하지 않는 크롤러도 있습니다.

robots.txt는 검색로봇에게 사이트 및 웹페이지를 수집할 수 있도록 허용하거나 제한하는 [국제 권고안](http://www.robotstxt.org/robotstxt.html)입니다. 참고로 2022년 3월 기준으로 IETF에서 [표준화 작업](https://datatracker.ietf.org/doc/html/draft-koster-rep)이 진행되고 있습니다.

robots.txt 파일은 항상 사이트의 루트 디렉터리에 위치해야 하며 [로봇 배제 표준](https://ko.wikipedia.org/wiki/%EB%A1%9C%EB%B4%87_%EB%B0%B0%EC%A0%9C_%ED%91%9C%EC%A4%80)을 따르는 일반 텍스트 파일로 작성해야 합니다. 네이버 검색로봇은 robots.txt에 작성된 규칙을 준수하며, 만약 사이트의 루트 디렉터리에 robots.txt 파일이 없다면 모든 콘텐츠를 수집할 수 있도록 간주합니다.

간혹 특정 목적을 위하여 개발된 웹 스크랩퍼를 포함하여 일부 불완전한 검색로봇은 robots.txt 내의 규칙을 준수하지 않을 수 있습니다. 그러므로 개인 정보를 포함하여 외부에 노출되면 안 되는 콘텐츠의 경우 로그인 기능을 통하여 보호하거나 다른 차단 방법을 사용해야 합니다.

[robots.txt 설정하기](https://searchadvisor.naver.com/guide/seo-basic-robots)

## 6. 🤔 CSR과 SSR의 차이를 SEO 관점에서 설명해주세요.

SSR : 페이지에 대한 meta 정보가 렌더링시 이미 포함되어 있어 크롤러봇에서 데이터를 수집해가는데 용이합니다.

CSR : 렌더링 시 JS 파싱, 로딩 및 실행 순서 때문에 크롤러봇이 데이터를 수집하는데 어려움이 있습니다.

[프론트엔드 면접 질문 리스트 - FE (Browser)](https://joontae-kim.github.io/2020/10/26/interview-question-fe/)

일반적으로 검색 엔진의 크롤러들은 데이터를 긁어올 때 웹 페이지의 JS를 해석해 노출시키기 때문에 크롤링을 할 수 없는 시점에서는 검색 엔진에 데이터를 노출시키지 않습니다. 이는 상대적으로 우리의 서비스가 검색 엔진 서비스에 노출되는 것이 줄어듦을 의미합니다.

따라서 CSR을 사용하면 View를 생성하기 위해서 JS가 필요하고(그 전까지는 빈 페이지이기 때문에 View가 완성되지 않아서), View를 생성하기 전까지는 검색 엔진 크롤러의 데이터 수집이 제한적이기 때문에 상대적으로 검색 엔진이 이해하는 정보가 부족해 SEO에 유리하지 않게 됩니다.

반대로 전통적인 SSR은 View를 서버에서 렌더링해 제공하기 때문에(View를 먼저 그리기 때문에) 상대적으로 SEO에 유리해져 사용자 유입이 많을 수 있습니다.

정리하면 전통적인 SSR은 초기 로딩 속도가 빠르고 SEO에 유리하지만, View 변경(화면 전환) 시 계속적으로(새로고침하며) 서버에 요청해야 하므로 서버에 부담이 큽니다. 그리고 CSR은 초기 로딩 속도가 느리고 SEO에 대한 문제가 있지만, 초기 로딩 후에는 View를 서버에 요청하는 것이 아닌 클라이언트에서 직접 렌더링하기 때문에 화면 전환이 매우 빠르다는 장점이 있고, 서버의 부담도 줄어듭니다.

@김서연 제가 이거 관련해서 꼬리 질문 받았던것중에

**그럼 과거에는 대부분의 페이지가 SSR 이었는데, CSR 로 넘어갈 수 있었던 이유 ? 기술적인 동기 ?**

- 브라우저 성능 향상 👍
- @정현수 SPA 라이브러리의 등장, 리액트 발명 등등...
- .. 등..

[서버사이드 렌더링 (개발자라면 상식으로 알고 있어야 하는 개념 정리 ⭐️)](https://www.youtube.com/watch?v=iZ9csAfU5Os)

### @정현수가 유튜브 보고 정리한 내용

- 2005년 AJAX를 사용해서 SPA를 구현하기 시작함
  - 컴퓨터의 성능이 좋아져서 많은 것들을 무리 없이 처리할 수 있게 됐다.
  - 브라우저가 성능이 향상이 됨
  - 자바스크립트도 표준화가 잘 됨에 따라서 강력한 커뮤니티를 바탕으로 CSR 시대로 접어 듦

## 7. 🤔 SPA에서 SEO에 유리하도록 만들기 위한 방법에 대해 설명해주세요.

첫 번째 페이지 로딩에서는 서버 사이드 렌더링을 사용하고, 그 후에 모든 페이지 로드에는 클라이언트 사이드 렌더링을 활용하는 방법을 많이 사용합니다.

React에서는 Next.js, GatsbyJS, Vue에서는 Nuxt.js 등의 라이브러리가 SPA에서 SEO을 할 수 있도록 도와줍니다.

많이 사용하는 Next.js의 경우 살펴보면 전통적인 SSR이 아닌 SPA에서 SEO에 유리하기 위한 SSR를 도입하고 그 이외에도 개발자들이 직접 노드에서 환경설정을 해주지 않고도 익숙한 툴(바벨, 웹팩 등)을 가지고 설정을 할 수 있게 지원해 주기에 많은 React 개발자들이 선호합니다.

또는 CSR에서 메타 태그를 정의해주는 라이브러리를 사용하는 것도 하나의 방법입니다. 대표적인 라이브러리로 react-helmet이 있습니다. 이 라이브러리는 동적으로 SEO에 필요한 메타태그들을 쉽게 변경할 수 있도록 도와주며 JSX 또는 TSX 내부에서 메타태그를 관리할 수 있습니다.

[FeBase/devowen_interview3.md at master · Febase/FeBase](https://github.com/Febase/FeBase/blob/master/interview/devowen_interview3.md)

## 8. 🤔 SEO 성능 향상 방법을 소개해주세요.

- `<title>` 태그를 이용하여 페이지별로 고유하고 정확한 페이지 제목 만들기
- `open graph` 적용하기
- 프로젝트에 sitemap.xml 생성해서 프로젝트 최상단에 위치시켜놓기
- 프로젝트에 robots.txt 생성해서 프로젝트 최상단에 위치시켜놓기
- `<meta name='description' content='~~~'>` 태그를 사용해 검색 엔진에 페이지 내용을 요약하여 제공하기
- `<h>` 태그를 사용해 중요한 텍스트 강조하기
- `<img>` 태그에 `alt` 속성을 추가해 어떤 이미지인지 설명 달기
- 구조화된 데이터 사용하기 → 구글 검색 페이지에 데이터가 보여질 가능성 올라감
- https 연결 사용하기
- 모바일도 지원하는 사이트 설계하기
- 웹 접근성을 준수하는 페이지 설계하기
- 논리적이고 의미있는 url 설정하기
  - ⭕ - example.com/RunningShoes/Womens
  - ❌ - example.com/123123/123123
- 동일한 컨텐츠들이 다른 URL에서 중복되지 않게 하기(동일한 내용을 보여준다면 하나의 링크로 고정시키기)
  - **`http://www.example.com/page/`**
     or **`http://www.example.com/page`**
     or **`http://www.example.com/page/index.html`** 중에 하나로 고정시키기
- `If-Modifed-Since` HTTP 헤더를 제대로 지원하는지 확인
  - 마지막 크롤링 이후 페이지 변경에 대한 정보를 제공
  - 대역폭과 오버헤드 줄일 수 있음

## 9. 🤔 검색엔진 작동 방식

- 크롤링 → 인덱싱 → 검색결과노출
- 크롤링 : 웹크롤러 자동화 소프트웨어가 페이지 탐색
- 인덱싱 : 탐색한 페이지에서 정보를 찾아 데이터베이스에 저장
- 검색결과노출 : 검색 알고리즘에 따라 최상의 검색 결과를 노출
